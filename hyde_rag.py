from langchain_groq import ChatGroq
from langchain_core.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter


# ---------- Step 1: PDF Preprocessing ----------
def replace_tabs_with_spaces(documents):
    """
    Replace all tab characters in the page_content of each document with spaces.
    """
    for doc in documents:
        doc.page_content = doc.page_content.replace("\t", " ")
    return documents


def build_vectorstore_from_pdf(pdf_path, chunk_size=1000, chunk_overlap=200):
    """
    Load a PDF and encode it into a FAISS vectorstore using HuggingFace embeddings.
    """
    loader = PyPDFLoader(pdf_path)
    raw_documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    split_documents = splitter.split_documents(raw_documents)
    cleaned_documents = replace_tabs_with_spaces(split_documents)

    embeddings_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(cleaned_documents, embeddings_model)
    return vectorstore


# ---------- Step 2: HyDE Retriever ----------
class HyDERetriever:
    def __init__(self, pdf_path, chunk_size=1000, chunk_overlap=200):
        """
        Initialize the retriever with PDF content, vectorstore, and Groq LLM.
        """
        self.llm_model = ChatGroq(model_name="qwen/qwen3-32b", groq_api_key="gsk_xxxx")

        self.vectorstore = build_vectorstore_from_pdf(pdf_path, chunk_size, chunk_overlap)
        self.chunk_size = chunk_size

        self.hyde_prompt = PromptTemplate(
            input_variables=["query", "chunk_size"],
            template="""
You are a helpful AI assistant. Based on the question '{query}', generate a hypothetical detailed answer.
The output should be approximately {chunk_size} characters long and useful for retrieving relevant documents.
""",
        )
        self.hyde_chain = self.hyde_prompt | self.llm_model

    def generate_hypothetical_doc(self, user_query):
        """
        Generate a hypothetical document that could answer the user's question.
        """
        return self.hyde_chain.invoke({"query": user_query, "chunk_size": self.chunk_size}).content

    def retrieve_documents(self, user_query, top_k=3):
        """
        Generate a hypothetical document and retrieve top_k most relevant documents from the vectorstore.
        """
        generated_hypothetical_doc = self.generate_hypothetical_doc(user_query)
        retrieved_docs = self.vectorstore.similarity_search(generated_hypothetical_doc, k=top_k)
        return retrieved_docs, generated_hypothetical_doc

    def answer_query(self, user_query, top_k=3):
        """
        Run HyDE-based retrieval + generation to produce a final answer.
        """
        retrieved_docs, generated_hypothetical_doc = self.retrieve_documents(user_query, top_k)
        context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])

        answer_prompt = PromptTemplate.from_template(
            """
You are an AI assistant skilled at answering questions based on document excerpts.
Use the context below to answer the user's question.

Context:
{context}

Question:
{question}

Answer in clear and concise English:
"""
        )

        final_prompt = answer_prompt.format(context=context_text, question=user_query)
        answer = self.llm_model.invoke(final_prompt)
        return answer.content, generated_hypothetical_doc, retrieved_docs


# ---------- Step 3: Main Script ----------
if __name__ == "__main__":
    pdf_path = "/Users/hzq/py_projects/hyde/Understanding_Climate_Change.pdf"
    user_question = "What is the main cause of climate change?"

    hyde_retriever = HyDERetriever(pdf_path=pdf_path)
    final_answer, hypothetical_doc, supporting_docs = hyde_retriever.answer_query(user_question)

    print("\n[ðŸ§  Hypothetical Document Generated by LLM]")
    print(hypothetical_doc)

    print("\n[ðŸ“„ Top Retrieved Document Snippets]")
    for idx, doc in enumerate(supporting_docs):
        print(f"\n--- Document {idx+1} ---\n{doc.page_content[:300]}...\n")

    print("\n[âœ… Final Answer Based on Retrieved Context]")
    print(final_answer)
